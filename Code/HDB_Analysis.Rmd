---
title: "HDB Price Analysis"
output:
  pdf_document: default
  html_document: default
date: "2024-04-30"
---

```{r}
#Importing Modules
library(dplyr)
library(tidyr)
library(lubridate)
library(fitdistrplus)
library(olsrr)
library(leaps)
library(boot)
library(glmnet)
library(randomForest)
library(tree)
library(ISLR)
library(zoo)
library(car)
library(knitr)
library(ggplot2)
library(dgof)
library(htmltools)
library(tinytex)
setwd("C:/Users/keith/OneDrive/Desktop/Github Projects/HDB Price Analysis")

#Data Pre-Processing
data_h = read.csv("HDB_Hougang_Final.csv", stringsAsFactors = TRUE)
data_s = read.csv("HDB_Serangoon_Final.csv", stringsAsFactors = TRUE)

#Data Wrangling
final_data = rbind(data_h, data_s)
head(final_data)

final_data = mutate(final_data, ID = row_number())
final_data = final_data %>% mutate(month = ym(as.character(month)))
final_data = final_data %>% mutate(remaining_lease = as.integer(substr(final_data[,"remaining_lease"], start = 1, stop = 2)))
final_data["Real_Value"] = (final_data[,"resale_price"]/final_data[,"Price.Index"])*100

#Removing redundant data
final_data = final_data %>% dplyr::select(month, ID, town, flat_type, storey_range, floor_area_sqm, flat_model, lease_commence_date, remaining_lease, Mrt_Distance, Real_Value)
head(final_data)

#[Look specific into storey_range & flat_model might have lack of data for specific points]

final_data %>% group_by(flat_model, town) %>% count() #[Remove adjoined flat due to low count + Remove redundant categories for fair comparison]
categories = c("Apartment", "Improved", "Maisonette", "Model A", "New Generation", "Simplified")
final_data = subset(final_data, flat_model %in% categories)

final_data %>% group_by(storey_range, town) %>% count() #[Remove 19 TO 21 due to lack of data points]
final_data = subset(final_data, !(storey_range %in% "19 TO 21"))
head(final_data)
summary(final_data)

#Removing Redundant Factors
final_data[, "storey_range"] = factor(final_data[, "storey_range"])
final_data[, "flat_model"] = factor(final_data[, "flat_model"])

#Renaming Columns
final_data = final_data %>% dplyr::rename(Town = town, Type = flat_type, Storey = storey_range, Area = floor_area_sqm, Model = flat_model, Lease_Begin = lease_commence_date, Lease_End = remaining_lease)

#Sectioning out Final Test Data
final_train = final_data %>% group_by(Town) %>% sample_frac(0.9, replace = FALSE)
final_test = anti_join(final_data, final_train)

#Sectioning out Validation Set
train = final_train %>% group_by(Town) %>% sample_frac(0.8, replace = FALSE)
test = anti_join(final_train, train)

#Remove months and ID
train = subset(train, select = -c(month, ID))
test = subset(test, select = -c(month, ID))

#General plot of all variables
pairs(train)

#Testing for Multicollinearity
#Area vs Type
summary(lm(Area~Type, data = train)) #[R^2 suggest high collinearity]
summary(lm(Real_Value ~ Type, data = train))
summary(lm(Real_Value ~ Area, data = train))
#[Both R^2 values are close, hence can test both for best model]

#Lease_Begin vs Lease_End
summary(lm(Lease_Begin ~ Lease_End, data = train)) #[R^2 suggest high collinearity]
summary(lm(Real_Value ~ Lease_Begin, data = train))
summary(lm(Real_Value ~ Lease_End, data = train))
#[Both R^2 values are close, hence can test both for best model]

#Area vs Model
summary(lm(Area~Model, data = train)) #[R^2 suggest ambiguous collinearity]
ols_vif_tol(lm(Real_Value ~ Area + Model, data = train)) #[Slight Correlation]

#Type vs Model
type_model = lm(Real_Value ~ Type + Model, data = train)
ols_vif_tol(type_model) #[High vif value indicates collinearity issue]

#Plot of individual variables against price
for (i in colnames(train[1:(length(train[1,])-1)])) {
  plot(train[[i]], train[["Real_Value"]], xlab = i, ylab = "Real Value", main = sprintf("Relationship between %s & Real Value", i))
}

#[Serangoon seems to display higher real value as opposed to Hougang symbolizing mature estate effect]
#[Strong linear trend in type of HDB and real value]
#[Slight linear trend in storey and real value]
#[Strong linear trend in area and real value]
#[General linear trend in model and real value if ordered by mean]
#[Ambiguous trends hence might be favorable to drop this for lease end]
#[Possibly a polynomial trend]
#[Very slight linear trend or polynomial trend]

#Base Relationship
for (i in colnames(train)) {
  print(summary(lm(paste("Real_Value ~", i), data = train)))
}
#[All variables shows statistical significance to 1% level however low explanatory power except Model, Area, Type]

#Polynomial Variations 
ggplot(train, aes(x = Mrt_Distance, y = Real_Value, color = Type)) + geom_point() + geom_smooth(formula = y ~ x)
ggplot(train, aes(Mrt_Distance, Real_Value, color = Type)) + geom_point() + geom_smooth(formula = y ~ poly(x,2))

ggplot(train, aes(Lease_End, Real_Value, color = Type)) + geom_point() + geom_smooth(formula = y ~ poly(x,2))
ggplot(train, aes(Lease_End, Real_Value, color = Type)) + geom_point() + geom_smooth(formula = y ~ x)

ggplot(train, aes(Area, Real_Value)) + geom_point() + geom_smooth(formula = y ~ poly(x,2))
ggplot(train, aes(Area, Real_Value)) + geom_point() + geom_smooth(formula = y ~ x)
#Poly function overfits to the training data hence better not to use it

#Best Subset Selection
RNGkind(sample.kind = "Rounding")
set.seed(6789)
k = 5

data_area = subset(train, select = -c(Type))
data_type = subset(train, select = -c(Area, Model))

predict.regsubsets <- function(object, newdata, id) {
  form <- as.formula(object$call[[2]])
  mat <- model.matrix(form, newdata)
  coefi <- coef(object, id = id)
  xvars <- names(coefi)
  mat[, xvars] %*% coefi
}

folds <- sample(1:k, nrow(data_area), replace = TRUE)
cv.errors <- matrix(NA, k, 14, dimnames = list(NULL, paste(1:14)))
for (j in 1:k){
  best.fit1 <- regsubsets(Real_Value ~ ., data = data_area[folds != j,], nvmax = 14) #Function for performing cross validation @ the j fold
  for (i in 1:14) {
    pred <- predict.regsubsets(best.fit1, data_area[folds ==j,], id = i) 
    cv.errors[j, i] <- mean((train$Real_Value[folds ==j] - pred)^2) #Function for making prediction for computing the MSE
  }
}
mean.cv1 <- apply(cv.errors, 2, mean) #apply function averages over the columns of the matrix 
aa <- which.min(mean.cv1)

folds <- sample(1:k, nrow(data_type), replace = TRUE)
cv.errors <- matrix(NA, k, 12, dimnames = list(NULL, paste(1:12)))
for (j in 1:k){
  best.fit2 <- regsubsets(Real_Value ~ ., data = data_type[folds != j,], nvmax = 12) #Function for performing cross validation @ the j fold
  for (i in 1:12) {
    pred <- predict.regsubsets(best.fit2, data_type[folds ==j,], id = i) 
    cv.errors[j, i] <- mean((train$Real_Value[folds ==j] - pred)^2) #Function for making prediction for computing the MSE
  }
}
mean.cv2 <- apply(cv.errors, 2, mean) #apply function averages over the columns of the matrix 
bb <- which.min(mean.cv2)

folds <- sample(1:k, nrow(data_area), replace = TRUE)
cv.errors <- matrix(NA, k, 16, dimnames = list(NULL, paste(1:16)))
for (j in 1:k){
  best.fit3 <- regsubsets(Real_Value ~ . - Mrt_Distance - Lease_End + poly(Mrt_Distance, 2) + poly(Lease_End,2), data = data_area[folds != j,], nvmax = 16) #Function for performing cross validation @ the j fold
  for (i in 1:16) {
    pred <- predict.regsubsets(best.fit3, data_area[folds ==j,], id = i) 
    cv.errors[j, i] <- mean((train$Real_Value[folds ==j] - pred)^2) #Function for making prediction for computing the MSE
  }
}
mean.cv3 <- apply(cv.errors, 2, mean) #apply function averages over the columns of the matrix 
cc <- which.min(mean.cv3)

folds <- sample(1:k, nrow(data_type), replace = TRUE)
cv.errors <- matrix(NA, k, 14, dimnames = list(NULL, paste(1:14)))
for (j in 1:k){
  best.fit4 <- regsubsets(Real_Value ~ . - Mrt_Distance - Lease_End + poly(Mrt_Distance, 2) + poly(Lease_End,2), data = data_type[folds != j,], nvmax = 14) #Function for performing cross validation @ the j fold
  for (i in 1:14) {
    pred <- predict.regsubsets(best.fit4, data_type[folds ==j,], id = i) 
    cv.errors[j, i] <- mean((train$Real_Value[folds ==j] - pred)^2) #Function for making prediction for computing the MSE
  }
}
mean.cv4 <- apply(cv.errors, 2, mean) #apply function averages over the columns of the matrix 
dd <- which.min(mean.cv4) #finds which regression model has the lowest MSE

c(mean.cv1[aa], mean.cv2[bb], mean.cv3[cc], mean.cv4[dd])

#First model is the best and all variables are used
coef(best.fit1, aa) #[Base Model MSE: 863774121]

#Interaction Effects
#Variables: Town, Storey, Area, Model, Lease_End, Mrt_Distance

#Initial Best Model
RNGkind(sample.kind = "Rounding")
set.seed(6789)

lr.mod1 = glm(Real_Value ~ . - Type, data = train)
cv.err1 = cv.glm(train, lr.mod1, K = 5)
cv.err1$delta[1] #[Initial RMSE]

#Town:Area#Town:Areacolours()
lr.mod2 = glm(Real_Value ~ . - Type + Town:Area, data = train)
cv.err2 = cv.glm(train, lr.mod2, K = 5)
cv.err2$delta[1] #[Large improvement in RMSE]

#Town:Model
lr.mod3 = glm(Real_Value ~ . - Type + Town:Area + Town:Model, data = train)
cv.err3 = cv.glm(train, lr.mod3, K = 5)
cv.err3$delta[1] #[Good improvement in RMSE]

#Storey:Area
lr.mod4 = glm(Real_Value ~ . - Type + Town:Area + Town:Model + Storey:Area, data = train)
cv.err4 = cv.glm(train, lr.mod4, K = 5)
cv.err4$delta[1] #[Marginal improvement in RMSE]

#Out-of-sample performance
test_var = subset(test, select = -c(Real_Value))
lr.pred = predict(lr.mod4, newdata = test_var)
mean((test$Real_Value - lr.pred)^2) #LR OOS Perf: 768447891

#Normalization
RNGkind(sample.kind = "Rounding")
set.seed(6789)

#Train-Test Generation
x.train <- model.matrix(Real_Value ~ . - Type + Town:Area + Town:Model + Storey:Area, data = train)[, -1]
y.train <- train$Real_Value

x.test <- model.matrix(Real_Value ~ . - Type + Town:Area + Town:Model + Storey:Area, data = test)[, -1]
y.test <- test$Real_Value

#Ridge Regression
ridge.mod <- cv.glmnet(x.train, y.train, alpha = 0)
plot(ridge.mod)
bestlam_ridge <- ridge.mod$lambda.min
ridge.pred <- predict(ridge.mod, s = bestlam_ridge, newx = x.test) 
mean((y.test - ridge.pred)^2)

#Lasso Regression
lasso.mod <- cv.glmnet(x.train, y.train, alpha = 1)
plot(lasso.mod)
bestlam_lasso <- lasso.mod$lambda.min
lasso.pred <- predict(lasso.mod, s = bestlam_lasso, newx = x.test)
mean((y.test - lasso.pred)^2)

#Ridge OOS Perf: 878373145
#Lasso OOS Perf: 774889012

#RandomForest
RNGkind(sample.kind = "Rounding")
set.seed(6789)

rf.mod <- randomForest(Real_Value ~ . - Type + Town:Area + Town:Model + Storey:Area, data = train, mtry = 6, importance = TRUE)
rf.pred = predict(rf.mod, newdata = test_var)
mean((test$Real_Value - rf.pred)^2) #[Greatly improved RMSE]
#RF OOS Perf: 427348172

#Test Predictions
RNGkind(sample.kind = "Rounding")
set.seed(6789)

final_test_var = subset(final_test, select = -c(month, ID, Type, Real_Value))
final_lr.mod = lm(Real_Value ~ . + Town:Area + Town:Model + Storey:Area, data = final_train[, -c(1,2,4,8)])
final_lr.pred = predict(final_lr.mod, newdata = final_test_var)
mean((final_test$Real_Value - final_lr.pred)^2)

final_rf.mod = randomForest(Real_Value ~ . + Town:Area + Town:Model + Storey:Area, data = final_train[, -c(1,2,4,8)], mtry = 6, importance = TRUE)
final_rf.pred = predict(final_rf.mod, newdata = final_test_var)
mean((final_rf.pred - final_test$Real_Value)^2)

#Final LR perf: 857393146
#Final RF perf: 450740537

#Ploting the Predictions
plot(final_rf.pred, final_test$Real_Value, col = c("blue","black"), pch = 20, xlab = "Predicted Value", ylab = "Actual Value", main = "Performance of RandomForest Model")
plot(final_lr.pred, final_test$Real_Value, col = c("red","black"), pch = 20, xlab = "Predicted Value", ylab = "Actual Value", main = "Performance of Linear Regression Model")
```